{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJ126V9kYdqA"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import time\n",
    "import collections\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 146432,
     "status": "ok",
     "timestamp": 1571817338998,
     "user": {
      "displayName": "Marcondes Coelho Feitoza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDiRRKmoIsQFILR3fLD_ZQ-w37ZvanCwNLKt29sNA=s64",
      "userId": "02119048484550635597"
     },
     "user_tz": 180
    },
    "id": "aOQ4J-lwZD1A",
    "outputId": "095638e2-f07c-4931-f844-5084fc6f7eb9"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd /content/drive/My Drive/Colab Notebooks/plantrevista/\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKANmrWYYw43"
   },
   "outputs": [],
   "source": [
    "\n",
    "def multi_clf_metrics(arq_csv):\n",
    "\n",
    "    # Cria X (features) e y (labels)\n",
    "    especies = pd.read_csv(arq_csv, low_memory=False)\n",
    "\n",
    "    if arq_csv == 'resnet50_features.csv' or arq_csv == 'resnet152v2_features.csv':\n",
    "      X = np.array(especies.iloc[0:,0:2048])\n",
    "      y = np.array(especies.iloc[0:,2048])\t\t\n",
    "    if arq_csv == 'nasnetlarge_features.csv':\n",
    "      X = np.array(especies.iloc[0:,0:4032])\n",
    "      y = np.array(especies.iloc[0:,4032])\t\t\n",
    "    if arq_csv == 'plantclef2013_inception_V3.csv':\n",
    "      X = np.array(especies.iloc[0:,0:2048])\n",
    "      y = np.array(especies.iloc[0:,2048])\t\t\n",
    "        \n",
    "    # Executa o PCA\n",
    "    \n",
    "    pca = PCA(.90)\n",
    "    principal_components = pca.fit_transform(X)\n",
    "\n",
    "    classifiers = {\n",
    "        'SVC - (polynomial, ovr)': OneVsRestClassifier(svm.SVC(kernel='poly', C=0.1, gamma=0.0001,degree=3, coef0=0.0,\n",
    "                                                               shrinking=True, probability=False, tol=0.001, cache_size=2000,\n",
    "                                                               class_weight=None, verbose=False, max_iter=-1,\n",
    "                                                               decision_function_shape='ovr', random_state=None))\n",
    "    }\n",
    "    scoring = ['accuracy',\n",
    "          'precision_macro','recall_macro','f1_macro',\n",
    "          'precision_micro','recall_micro','f1_micro',\n",
    "          'precision_weighted','recall_weighted','f1_weighted']\n",
    "    for name_clf, clf in classifiers.items():\n",
    "      \n",
    "      inicio = time.time()\n",
    "      \n",
    "      #Variáveis do Dataset de Resultados\n",
    "      arquivo_csv = []\n",
    "      classifier = []\n",
    "      accuracy = []\n",
    "      std_accuracy = []\n",
    "\n",
    "      # Metrics Macro\n",
    "      precision_macro = []\n",
    "      std_precision_macro = []\n",
    "      recall_macro = []\n",
    "      std_recall_macro = []\n",
    "      f1_macro = []\n",
    "      std_f1_macro = []\n",
    "\n",
    "      # Metrics Micro\n",
    "      precision_micro = []\n",
    "      std_precision_micro = []\n",
    "      recall_micro = []\n",
    "      std_recall_micro = []\n",
    "      f1_micro = []\n",
    "      std_f1_micro = []\n",
    "\n",
    "      # Metrics Weighted\n",
    "      precision_weighted = []\n",
    "      std_precision_weighted = []\n",
    "      recall_weighted = []\n",
    "      std_recall_weighted = []\n",
    "      f1_weighted = []\n",
    "      std_f1_weighted = []\n",
    "\n",
    "      # Informações do PCA\n",
    "      n_samples = []\n",
    "      princ_compo = []\n",
    "      variancia = []\n",
    "\n",
    "      print(f'Executando classificador \\033[1;31m{name_clf}\\033[m')\n",
    "      classifier.append(name_clf)\n",
    "      arquivo_csv.append(arq_csv[:-4])\n",
    "      scores = cross_validate(clf, principal_components, y, scoring=scoring, cv=10, return_train_score=False,n_jobs=-1, verbose=51)\n",
    "\n",
    "      accuracy.append(scores['test_accuracy'].mean())\n",
    "      std_accuracy.append(scores['test_accuracy'].std())\n",
    "\n",
    "      # Metrics Macro\n",
    "      precision_macro.append(scores['test_precision_macro'].mean())\n",
    "      std_precision_macro.append(scores['test_precision_macro'].std())\n",
    "      recall_macro.append(scores['test_recall_macro'].mean())\n",
    "      std_recall_macro.append(scores['test_recall_macro'].std())\n",
    "      f1_macro.append(scores['test_f1_macro'].mean())\n",
    "      std_f1_macro.append(scores['test_f1_macro'].std())\n",
    "      print(f'\\033[1;32mMétricas Macro extraidas!\\033[m')\n",
    "\n",
    "      # Metrics Micro\n",
    "      precision_micro.append(scores['test_precision_micro'].mean())\n",
    "      std_precision_micro.append(scores['test_precision_micro'].std())\n",
    "      recall_micro.append(scores['test_recall_micro'].mean())\n",
    "      std_recall_micro.append(scores['test_recall_micro'].std())\n",
    "      f1_micro.append(scores['test_f1_micro'].mean())\n",
    "      std_f1_micro.append(scores['test_f1_micro'].std())\n",
    "      print(f'\\033[1;32mMétricas Micro extraidas!\\033[m')\n",
    "\n",
    "      # Metrics Weighted\n",
    "      precision_weighted.append(scores['test_precision_weighted'].mean())\n",
    "      std_precision_weighted.append(scores['test_precision_weighted'].std())\n",
    "      recall_weighted.append(scores['test_recall_weighted'].mean())\n",
    "      std_recall_weighted.append(scores['test_recall_weighted'].std())\n",
    "      f1_weighted.append(scores['test_f1_weighted'].mean())\n",
    "      std_f1_weighted.append(scores['test_f1_weighted'].std())\n",
    "      print(f'\\033[1;32mMétricas Weighted extraidas!\\033[m')\n",
    "\n",
    "      n_samples.append(principal_components.shape[0])\n",
    "      princ_compo.append(principal_components.shape[1])\n",
    "      variancia.append(sum(pca.explained_variance_ratio_))\n",
    "\n",
    "      print(f'Base {arq_csv[:-4]} \\033[1;32mOK!\\033[m')\n",
    "      print('=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=')\n",
    "      print('')\n",
    "      \n",
    "      # Criando um DataFrame com os Resultados\n",
    "      data = {'Dataset': arquivo_csv, 'Classificador': classifier, 'Accuracy': accuracy,'D.P. - Acurácia': std_accuracy, \n",
    "          'Precision Macro': precision_macro, 'D.P - Precision Macro': std_precision_macro,\n",
    "          'Recall Macro': recall_macro, 'D.P - Recall Macro': std_recall_macro,\n",
    "          'F1 Macro': f1_macro, 'D.P - F1 Macro': std_f1_macro,\n",
    "          'Precision Micro': precision_micro, 'D.P - Precision Micro': std_precision_micro,\n",
    "          'Recall Micro': recall_micro, 'D.P - Recall Micro': std_recall_micro,\n",
    "          'F1 Micro': f1_micro, 'D.P - F1 Micro': std_f1_micro,\n",
    "          'Precision Weighted': precision_weighted, 'D.P - Precision Weighted': std_precision_weighted,\n",
    "          'Recall Weighted': recall_weighted, 'D.P - Recall Weighted': std_recall_weighted,\n",
    "          'F1 Weighted': f1_weighted, 'D.P - F1 Weighted': std_f1_weighted,\n",
    "          'N. Amostras': n_samples, 'PCA': princ_compo, 'Variância': variancia}\n",
    "      df = pd.DataFrame(data)\n",
    "      df.to_csv('resultados/'+name_clf+'_'+arq_csv[:-4]+'.csv', index=None)\n",
    "      \n",
    "      fim = time.time()\n",
    "      print(f'Tempo de Execução: {(fim - inicio)/3600} horas')\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25514515,
     "status": "ok",
     "timestamp": 1571842973744,
     "user": {
      "displayName": "Marcondes Coelho Feitoza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDiRRKmoIsQFILR3fLD_ZQ-w37ZvanCwNLKt29sNA=s64",
      "userId": "02119048484550635597"
     },
     "user_tz": 180
    },
    "id": "KqBPDUZSYw20",
    "outputId": "4e5b8a18-fc6d-4938-ae5d-a1ab9533f16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando classificador \u001b[1;31mSVC - (polynomial, ovr)\u001b[m\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Memmapping (shape=(26077, 274), dtype=float64) to new file /var/folders/k2/7m7wfg6n6y5fylg_14w25lnc0000gp/T/joblib_memmapping_folder_24051_6741672734/24051-4344023416-4e95f6eb4c6f4e179172126a4c01c1dd.pkl\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed: 23.0min remaining: 92.1min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed: 23.1min remaining: 53.9min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed: 23.2min remaining: 34.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed: 23.2min remaining: 23.2min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed: 23.3min remaining: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed: 23.3min remaining: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed: 23.3min remaining:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 34.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 34.7min finished\n",
      "\u001b[1;32mMétricas Macro extraidas!\u001b[m\n",
      "\u001b[1;32mMétricas Micro extraidas!\u001b[m\n",
      "\u001b[1;32mMétricas Weighted extraidas!\u001b[m\n",
      "Base resnet152v2_features \u001b[1;32mOK!\u001b[m\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "\n",
      "Tempo de Execução: 0.5790037635962169 horas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_clf_metrics('resnet152v2_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Chs0mC9PYw0K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of marcondes.ipynb",
   "provenance": [
    {
     "file_id": "13oGvF8L8X3DTNoOIxu8LITJn8jNRYioy",
     "timestamp": 1573952282733
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
